\documentclass{beamer}
\usetheme{Boadilla}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}

\setlength {\marginparwidth }{2cm}

\def\rnum{\mathbb{R}}

\title{Convex Optimisation}
\author{Alessio Zakaria}
\date{}
\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Convex Optimisation?}
    \begin{itemize}
    \item What is it?
        \begin{itemize}
        \item Finding the maxima / minima of convex functions over convex sets
        with respect to convex or affine constraints
        \end{itemize}
    \item Why do we care?
        \begin{itemize}
            \item Convex functions display many theoretical properties that are
                suited to optimisation
            \item Solving convex optimisation allows you to place a lower bound
                on non-convex functions
            \item Many real world optimisations are convex
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Convexity}
    A set $C \subseteq \mathbb{R}$ is convex if $\forall \,  x,  y \in C, \,
    \forall \, \theta \in [0, \, 1]$
    \begin{align*}
        \theta x + (1-\theta)y \in C
    \end{align*}
    i.e. all points on the line segment between $x$ and $y$ lie in C
    \todo{add affine definition}
\end{frame}

\begin{frame}
    \frametitle{Minima and Maxima}
    $f: S \subseteq \rnum^{n} \rightarrow \rnum$ has a global minimum (maximum
    resp.) $x^{\star}$ if:
    \begin{align*}
        \forall \, x \in S, \, \, f(x^{\star}) \leq f(x), \hspace{2mm}  (f(x^{\star}) \geq f(x))
    \end{align*}
    \\~\\
    $f$ has a local minimum (maximum resp.) around $x^{\star}$ if $\exists \, R \in
     \rnum$ \text{ such that }
    \begin{align*}
        \forall x \, \in B(x^{\star}, R), \, \, f(x^{\star}) \leq f(x),
        \hspace{2mm} (f(x^{\star}) \geq f(x))
    \end{align*}
    \todo{add strict definitions}
\end{frame}

\begin{frame}
    \frametitle{Convex Optimisation Problem}
    A convex optimisation problem is a min/maximisation problem in the following
    form:
    {\footnotesize
    \begin{align*}
        \text{Find the }\underset{x}{\text{min}}\; &f_{0}(x) \\
        \text{  such that }&f_{1}(x) \leq 0 \\
        & \hspace{5mm} \vdots \\
        &f_{n}(x) \leq 0 \\
        &g_{0}(x) = 0 \\
        &\hspace{5mm} \vdots \\
        &g_{m}(x) = 0
    \end{align*}}
    \\~\\
    Where $f_0,\cdots, f_n$ are convex functions $\rnum^{p} \rightarrow \rnum$ and $g_0, \cdots, g_m$ are
    affine $\rnum^{p} \rightarrow \rnum$.
\end{frame}

\begin{frame}
    \frametitle{Feasible Set}
    The feasible set $C \subseteq D =
    \bigcap\limits_{i=0}^{m}\text{dom}(f_{i})\bigcap\limits_{j=0}^{n}\text{dom}(g_{j})$
    is the set of all $x \in D$ such that the constraints are satisfied.
    \\~\\
    The optimal value of problem is $\text{inf} \{f_{0}(x) \, | \, x
    \in C \}$. If there is no lower bound the optimisation problem has optimal
    value $-\infty$.
\end{frame}

\begin{frame}
    \frametitle{Feasible Set}
    Some important propterties of convexity are:
    \begin{itemize}
        \item The intersection of convex sets is convex
        \item The sublevel sets ($\{x \, | \, f(x) \leq 0 \}$) of convex
            functions are convex
        \item The image of affine functions is convex
    \end{itemize}
    \todo{check affine function comment}
    Because of this the feasible set of of a convex optimisation problem is
    convex.
\end{frame}

\begin{frame}
    \frametitle{Convexity and Optimality}
    The above is important because of the following key fact:
    \\~\\
    Any local minimum of a convex function over a convex set is a global
    minimum
    \todo{Do this proof in the extended material}
    This means that any algorithm for a convex optimisation problem only has to
    find a local minumum.
    \\~\\
    This guarantees that algorithms which are only
    guaranteed to find local minima for all problems provide global minima for
    convex problems.
\end{frame}

\begin{frame}
    First order and Second order sufficient conditions for convexity and
    optimiality
\end{frame}


\begin{frame}
    \frametitle{Duality and the Langrangian}
    For any optimisation problem there is a related problem known as the dual
    (with the original problem known as the primal).
    \\~\\
    The solution to the dual provides some information about the solution to the
    primal. The dual problem is related to the Lagrangian function of the
    original problem.
    \\~\\
    The dual problem is always convex and therefore being able to solve
    convex optimisation problems allows us to solve the dual.
\end{frame}

\begin{frame}
    \frametitle{The Lagrangian}
    Consider some optimisation problem:
    {\footnotesize
    \begin{align*}
        \text{Find the }\underset{x}{\text{min}}\; &f_{0}(x) \\
        \text{  such that }&f_{1}(x) \leq 0 \\
        & \hspace{5mm} \vdots \\
        &f_{n}(x) \leq 0 \\
        &g_{0}(x) = 0 \\
        &\hspace{5mm} \vdots \\
        &g_{m}(x) = 0
    \end{align*}}
    Where $f_0,\cdots, f_n, g_0, \cdots, g_m:\, \rnum^{p} \rightarrow \rnum$
\end{frame}

\begin{frame}
    \frametitle{The Lagrangian}
    The Lagrangian, $L : \rnum^{p} \times \rnum^{n} \times \rnum^{m} \rightarrow
    \rnum$, of this optimisation problem is the function:
    \begin{align*}
        L(x, \lambda, \mu) = f_{0}(x) + \sum\limits_{i=1}^{n}\lambda_{i} f_{i}(x) +
        \sum\limits_{j=0}^{m}\mu_{j}g_{j}(x)
    \end{align*}
    The parameters $\lambda$ and $\mu$ are known as the Lagrangian multipliers
\end{frame}

\begin{frame}
    \frametitle{The Dual Function}
    The Dual Function $L^{\star}: \rnum^{n}\times\rnum^{m} \rightarrow \rnum$ is
    the function
    \begin{align*}
        L^{\star} = \underset{x \in D}{\text{min}}L(x, \lambda, \mu)
    \end{align*}
    For the optimal value of the primal problem, $p^{\star}$, for all values
    $\lambda \geq 0$
    \begin{align*}
        g(\lambda, \mu) \leq p^{\star}
    \end{align*}
    \todo{proof in notes}
\end{frame}

\begin{frame}
    \frametitle{The Dual Problem}
    Given that the values of the dual function lower bound the optimal solution
    for the primal we can formulate the question of what the greatest lower
    bound is.
    \\~\\
    This leads to the optimisation problem.
    {\footnotesize
    \begin{align*}
        \text{maximise } \, &L^{\star}(\lambda, \mu)\\
        \text{such that } &\lambda \geq 0
    \end{align*}}
    With optimal value $g^{\star} \leq p^{\star}$. \end{frame}

\begin{frame}
    \frametitle{Duality}
    Regardless of whether the primal problem is convex the dual problem is
    \textit{always} convex.
    \\~\\
    This means that if convex optimisation problems can be solved efficiently
    then a lower bound can be placed on any optimisation problem that may be
    harder to solve.
    \\~\\
    This can be used in and of itself but can also be used to derive stopping
    conditions on algorithms aiming to solve the primal.
\end{frame}

\begin{frame}
    \frametitle{Solving Convex Optimisation Problems}
    One method of solving convex optimisation problems is using interior point
    methods.
    \\~\\
    Intuition: Reformulate the problem as an equality constrained problem which
    can be solved via Newton's Method
\end{frame}

\begin{frame}
    \frametitle{Newton's Method}
    Newton's method is a method for solving unconstrained minimisation problems
    on twice differentiable functions that makes use of the Hessian to take into
    account local curvature information to more quickly reach the minimum.
\end{frame}

\begin{frame}
    \frametitle{Newton's Method for Equality Constrained Optimisation}
    \todo{here}
\end{frame}

\begin{frame}
    \frametitle{Inequalities into Equalities}
    We incorporate the inequalities into the objective using an indicator
    function:
    \begin{align*}
        \text{minimise } \, &f_{0} +
        \sum_{i=1}^{m}\mathcal{I}\_\left(f_{i}\,(x)\right )\\
        \text{such that } & \mathbf{A}\mathbf{x} =\mathbf{b}
    \end{align*}

where

\begin{align*}
    \mathcal{I}\_(u) =
    \begin{cases}
        0 &u \leq 0 \\
        \infty &u > 0
    \end{cases}
\end{align*}
\end{frame}

\begin{frame}
    \frametitle{Inequalities into Equalities}
    This objective is not differentiable and therefore Newton's method cannot be
    applied.
    \\~\\
    To rectify this we can use an approximation using the log barrier function.
    \begin{align*}
        \hat{\mathcal{I}}\_\, (u) = -\frac{1}{t}\, \text{log}\,(-u) \\\\
        \mathbf{dom} \,\,\,\hat{\mathcal{I}}\_ = -\rnum^{++}
    \end{align*}
    Where $t$ is a parameter that is set to adjust the accuracy of the barrier
    function
\end{frame}

\begin{frame}
    \frametitle{Log Barrier Function}
    The minimisation problem can be reformuated as the following:
    \begin{align*}
        \text{minimise } \, &t f_{0} +
        \varphi(x)\\
        \text{such that } & \mathbf{A}\mathbf{x} =\mathbf{b}
    \end{align*}
    where
    \begin{align*}
        \varphi(x) = - \sum\limits_{i=1}^{m}\text{log}(-f_{i}(x))
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Log Barrier Function}
    \todo{picture of log barrier}
    The log barrier function is twice continuously differentiable and convex
    and therefore is amenable to Newton's method.
\end{frame}


\begin{frame}
    \frametitle{Central Path}
    Assuming the log barrier formalisation has a unique solution for all
    $t > 0$ obtainable via Newton's method, we can define the central path,
    $x^{\star}(t)$ as the solution to the minimisation problem for a given
    $t$.
\end{frame}

\begin{frame}
    \frametitle{Central Path}
    From the KKT conditions a dual feasible point for every
    $x^{\star}(t)$ can be obtained which lower bounds the optimal value for
    the original problem.
    \\~\\
    For a given $x^{\star}(t)$, $f_{0}(x^{\star}(t)$ is no more than
    $\frac{m}{t}$ away from the optimal value for the original primal problem.
    \\~\\
    As $t \rightarrow \infty$ the central path solution will tend to
    $p^{\star}$.
\end{frame}

\begin{frame}
    \frametitle{Barrier Method}
    Given that an abitrary $x^{\star}(t)$ is $\frac{m}{t}$ suboptimal a method of
    finding the minimum of the original problem to a specific accuracy,
    $\varepsilon$ is by solving the problem:
    \begin{align*}
        \text{minimise } \, &\left(\frac{m}{\varepsilon}\right)\,f_{0} +
        \varphi(x)\\
        \text{such that } & \mathbf{A}\mathbf{x} =\mathbf{b}
    \end{align*}
    This can be solved using Newton's method however it does not have good
    practical performance due to numerical instability.
    \todo{eps needs to be big}
\end{frame}

\begin{frame}
    \frametitle{Barrier Method}
    The barrier method avoids the problems of the previous method which solves a
    series of minimisation problems.
    \\~\\
    The barrier method computes $x^{\star}(t)$ for increasing values of
    $t$ using Newton's method until $t \geq \frac{m}{\varepsilon}$.
\end{frame}

\begin{frame}
    \frametitle{Barrier Method Algorithm}
\end{frame}

\begin{frame}
    \frametitle{Barrier Method Algorithm}
\end{frame}

\begin{frame}
    \frametitle{Convergence of Barrier Method}
\end{frame}


\begin{frame}
    \frametitle{Applications}
\end{frame}
% \begin{frame}
%     \frametitle{Strong and Weak Duality}
% \end{frame}

% \begin{frame}
%     \frametitle{Strong Duality for Convex Problems}
% \end{frame}

% \begin{frame}
%     \frametitle{Solving Convex Optimisation Problems}
% \end{frame}
\end{document}

